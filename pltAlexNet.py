import matplotlib.pyplot as plt
import matplotlib as mpl

mpl.rcParams["font.sans-serif"]=["SimHei"]
mpl.rcParams["axes.unicode_minus"] = False

epoch = []

for i in range(1,51):
    epoch.append(i)

loss05=[0.851, 0.51, 0.395, 0.359, 0.346, 0.3, 0.267, 0.263, 0.27, 0.259, 0.237, 0.233, 0.22, 0.195, 0.202, 0.199, 0.18, 0.201, 0.184, 0.212, 0.174, 0.196, 0.193, 0.17, 0.182, 0.178, 0.18, 0.176, 0.162, 0.166, 0.155, 0.147, 0.261, 0.172, 0.166, 0.148, 0.152, 0.137, 0.155, 0.172, 0.147, 0.124, 0.139, 0.132, 0.168, 0.144, 0.141, 0.13, 0.124, 0.157]
loss1 = [0.769, 0.484, 0.389, 0.337, 0.305, 0.292, 0.243, 0.24, 0.233, 0.301, 0.244, 0.205, 0.221, 0.199, 0.218, 0.192, 0.231, 0.194, 0.182, 0.183, 0.175, 0.17, 0.162, 0.16, 0.177, 0.163, 0.159, 0.173, 0.182, 0.168, 0.172, 0.17, 0.173, 0.161, 0.158, 0.145, 0.158, 0.152, 0.138, 0.147, 0.151, 0.189, 0.139, 0.133, 0.139, 0.146, 0.141, 0.137, 0.131, 0.133]
loss2 = [0.846, 0.506, 0.421, 0.392, 0.311, 0.293, 0.266, 0.233, 0.243, 0.231, 0.236, 0.222, 0.206, 0.196, 0.241, 0.191, 0.192, 0.187, 0.208, 0.169, 0.189, 0.17, 0.165, 0.166, 0.157, 0.173, 0.153, 0.171, 0.157, 0.142, 0.147, 0.145, 0.154, 0.162, 0.127, 0.144, 0.149, 0.148, 0.178, 0.139, 0.183, 0.164, 0.148, 0.125, 0.121, 0.124, 0.132, 0.132, 0.126, 0.131]
loss3 = [0.867, 0.547, 0.39, 0.352, 0.319, 0.277, 0.275, 0.286, 0.24, 0.205, 0.229, 0.265, 0.206, 0.206, 0.199, 0.177, 0.179, 0.175, 0.2, 0.193, 0.167, 0.174, 0.171, 0.155, 0.192, 0.169, 0.167, 0.16, 0.162, 0.139, 0.153, 0.193, 0.163, 0.152, 0.146, 0.138, 0.143, 0.15, 0.144, 0.134, 0.141, 0.148, 0.137, 0.133, 0.147, 0.141, 0.129, 0.127, 0.14, 0.134]
loss4 = [0.841, 0.478, 0.42, 0.368, 0.311, 0.275, 0.27, 0.242, 0.218, 0.213, 0.244, 0.236, 0.209, 0.19, 0.249, 0.196, 0.183, 0.176, 0.177, 0.2, 0.176, 0.163, 0.168, 0.188, 0.168, 0.159, 0.163, 0.152, 0.207, 0.165, 0.146, 0.153, 0.143, 0.142, 0.197, 0.157, 0.139, 0.139, 0.142, 0.141, 0.128, 0.158, 0.139, 0.147, 0.132, 0.127, 0.134, 0.127, 0.127, 0.148]
loss5=[0.741, 0.421, 0.389, 0.338, 0.314, 0.32, 0.255, 0.25, 0.242, 0.217, 0.216, 0.21, 0.217, 0.203, 0.189, 0.198, 0.171, 0.167, 0.181, 0.165, 0.176, 0.216, 0.179, 0.16, 0.164, 0.175, 0.169, 0.154, 0.143, 0.14, 0.154, 0.152, 0.17, 0.156, 0.148, 0.152, 0.139, 0.152, 0.156, 0.133, 0.13, 0.141, 0.16, 0.134, 0.139, 0.136, 0.147, 0.149, 0.133, 0.132]
loss6=[0.765, 0.472, 0.399, 0.334, 0.281, 0.288, 0.268, 0.261, 0.237, 0.234, 0.219, 0.211, 0.202, 0.203, 0.178, 0.193, 0.188, 0.172, 0.173, 0.186, 0.19, 0.173, 0.165, 0.175, 0.242, 0.173, 0.172, 0.161, 0.159, 0.16, 0.17, 0.152, 0.151, 0.145, 0.154, 0.134, 0.152, 0.189, 0.128, 0.13, 0.119, 0.143, 0.136, 0.153, 0.122, 0.129, 0.155, 0.173, 0.154, 0.141]
loss7=[0.751, 0.446, 0.378, 0.331, 0.294, 0.284, 0.255, 0.222, 0.234, 0.222, 0.213, 0.218, 0.201, 0.206, 0.207, 0.193, 0.185, 0.195, 0.164, 0.23, 0.167, 0.175, 0.176, 0.155, 0.157, 0.153, 0.166, 0.184, 0.179, 0.146, 0.149, 0.159, 0.148, 0.136, 0.154, 0.137, 0.157, 0.16, 0.153, 0.162, 0.172, 0.123, 0.121, 0.128, 0.14, 0.138, 0.137, 0.124, 0.124, 0.118]
acc7=[0.783, 0.842, 0.876, 0.883, 0.903, 0.929, 0.93, 0.936, 0.942, 0.932, 0.948, 0.948, 0.946, 0.943, 0.952, 0.953, 0.946, 0.959, 0.957, 0.959, 0.942, 0.955, 0.949, 0.957, 0.969, 0.969, 0.963, 0.965, 0.975, 0.965, 0.966, 0.967, 0.977, 0.971, 0.972, 0.964, 0.961, 0.971, 0.972, 0.965, 0.969, 0.969, 0.969, 0.97, 0.976, 0.982, 0.975, 0.967, 0.967, 0.982]

loss7eca=[0.829, 0.538, 0.405, 0.353, 0.31, 0.284, 0.259, 0.229, 0.265, 0.238, 0.234, 0.199, 0.219, 0.183, 0.22, 0.192, 0.205, 0.196, 0.182, 0.174, 0.158, 0.179, 0.198, 0.173, 0.167, 0.165, 0.167, 0.148, 0.183, 0.157, 0.15, 0.134, 0.207, 0.155, 0.151, 0.179, 0.154, 0.147, 0.137, 0.143, 0.144, 0.123, 0.124, 0.146, 0.145, 0.121, 0.13, 0.145, 0.139, 0.129]
acc7eca=[0.808, 0.873, 0.836, 0.928, 0.929, 0.942, 0.865, 0.937, 0.955, 0.948, 0.952, 0.942, 0.959, 0.963, 0.945, 0.932, 0.938, 0.93, 0.961, 0.961, 0.955, 0.958, 0.966, 0.961, 0.955, 0.964, 0.959, 0.945, 0.971, 0.964, 0.973, 0.979, 0.961, 0.972, 0.979, 0.918, 0.973, 0.977, 0.952, 0.966, 0.978, 0.982, 0.952, 0.97, 0.965, 0.957, 0.97, 0.97, 0.971, 0.977]

loss7cbam=[0.906, 0.533, 0.433, 0.373, 0.341, 0.317, 0.411, 0.289, 0.257, 0.268, 0.241, 0.247, 0.211, 0.214, 0.208, 0.183, 0.224, 0.203, 0.173, 0.207, 0.185, 0.191, 0.174, 0.173, 0.166, 0.164, 0.192, 0.158, 0.17, 0.142, 0.152, 0.146, 0.246, 0.18, 0.163, 0.154, 0.139, 0.141, 0.14, 0.139, 0.124, 0.132, 0.132, 0.166, 0.16, 0.145, 0.132, 0.139, 0.124, 0.123]
acc7cbam=[0.843, 0.783, 0.871, 0.855, 0.905, 0.911, 0.902, 0.93, 0.93, 0.941, 0.941, 0.951, 0.955, 0.96, 0.963, 0.961, 0.949, 0.971, 0.952, 0.902, 0.949, 0.953, 0.965, 0.969, 0.961, 0.954, 0.96, 0.965, 0.975, 0.947, 0.967, 0.957, 0.96, 0.965, 0.958, 0.976, 0.958, 0.967, 0.963, 0.957, 0.97, 0.97, 0.964, 0.932, 0.971, 0.964, 0.976, 0.967, 0.96, 0.976]


AlexNetacc = [91.91,92.27,93.38,92.65,92.52,92.40,92.28,93.01]
SEAlexNetacc = [87.99,88.73,91.78,93.75,95.34,95.83,95.83,96.32]
sampleRate=[0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7]

lr001loss=[1.358, 1.173, 1.071, 1.023, 0.824, 0.828, 0.63, 0.532, 0.533, 0.54, 0.448, 0.404, 0.436, 0.321, 0.329, 0.258, 0.396, 0.425, 0.341, 0.243, 0.239, 0.421, 0.281, 0.252, 0.228, 0.258, 0.195, 0.184, 0.211, 0.273, 0.182, 0.191, 0.285, 0.208, 0.208, 0.276, 0.461, 0.256, 0.208, 0.219, 0.201, 0.437, 0.56, 0.289, 0.279, 0.177, 0.184, 0.191, 0.228, 0.311]
lr001acc=[0.253, 0.596, 0.391, 0.522, 0.726, 0.761, 0.757, 0.866, 0.792, 0.911, 0.924, 0.932, 0.88, 0.95, 0.971, 0.965, 0.907, 0.919, 0.956, 0.974, 0.963, 0.958, 0.934, 0.968, 0.949, 0.976, 0.974, 0.975, 0.974, 0.974, 0.966, 0.973, 0.971, 0.966, 0.99, 0.984, 0.983, 0.969, 0.945, 0.974, 0.963, 0.664, 0.971, 0.963, 0.963, 0.983, 0.979, 0.99, 0.974, 0.953]
lr0001loss=[1.194, 0.811, 0.525, 0.387, 0.291, 0.261, 0.18, 0.186, 0.14, 0.156, 0.152, 0.164, 0.146, 0.116, 0.121, 0.094, 0.116, 0.104, 0.126, 0.11, 0.084, 0.076, 0.07, 0.093, 0.092, 0.074, 0.08, 0.063, 0.092, 0.081, 0.064, 0.08, 0.088, 0.051, 0.086, 0.07, 0.077, 0.056, 0.075, 0.047, 0.048, 0.085, 0.068, 0.066, 0.064, 0.057, 0.06, 0.067, 0.077, 0.037]
lr0001acc=[0.639, 0.838, 0.906, 0.93, 0.971, 0.974, 0.981, 0.986, 0.991, 0.975, 0.978, 0.966, 0.996, 0.99, 0.993, 0.971, 0.994, 0.99, 0.996, 0.985, 0.99, 0.998, 0.993, 0.994, 0.991, 0.995, 0.995, 0.993, 0.996, 0.994, 0.998, 0.995, 0.996, 0.996, 0.995, 0.993, 0.994, 0.974, 0.999, 0.996, 0.994, 0.99, 0.999, 0.991, 0.999, 0.989, 0.994, 0.985, 0.988, 0.999]
lr00001loss=[1.382, 1.354, 1.186, 0.986, 0.894, 0.821, 0.771, 0.738, 0.707, 0.658, 0.623, 0.625, 0.584, 0.554, 0.534, 0.478, 0.468, 0.442, 0.411, 0.397, 0.371, 0.341, 0.319, 0.314, 0.327, 0.294, 0.274, 0.284, 0.252, 0.252, 0.256, 0.244, 0.233, 0.213, 0.221, 0.198, 0.194, 0.197, 0.186, 0.189, 0.179, 0.178, 0.174, 0.167, 0.148, 0.164, 0.163, 0.14, 0.163, 0.137]
lr00001acc=[0.25, 0.394, 0.535, 0.637, 0.705, 0.718, 0.751, 0.791, 0.802, 0.814, 0.834, 0.844, 0.866, 0.881, 0.902, 0.917, 0.92, 0.944, 0.934, 0.931, 0.945, 0.936, 0.954, 0.915, 0.938, 0.941, 0.955, 0.97, 0.96, 0.963, 0.971, 0.961, 0.943, 0.949, 0.966, 0.975, 0.976, 0.97, 0.974, 0.964, 0.963, 0.98, 0.973, 0.971, 0.983, 0.98, 0.976, 0.984, 0.98, 0.98]



plt.plot(epoch,lr001acc,label="lr=0.001")
plt.plot(epoch,lr0001acc,label="lr=0.0001")
plt.plot(epoch,lr00001acc,label="lr=0.00001")



#plt.title('Accuracy of each sampling rate')
plt.title('Comparison of different learning rates')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.legend()
plt.show()