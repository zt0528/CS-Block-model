import matplotlib.pyplot as plt

epoch = []

for i in range(1,51):
    epoch.append(i)

#resnet50+se
loss05=[1.175, 1.068, 1.003, 0.958, 0.928, 0.917, 0.891, 0.857, 0.836, 0.82, 0.812, 0.802, 0.776, 0.779, 0.773, 0.749, 0.751, 0.728, 0.739, 0.72, 0.722, 0.705, 0.7, 0.691, 0.698, 0.679, 0.673, 0.691, 0.682, 0.668, 0.664, 0.666, 0.647, 0.638, 0.641, 0.652, 0.637, 0.62, 0.636, 0.63, 0.616, 0.627, 0.605, 0.608, 0.621, 0.614, 0.605, 0.605, 0.598, 0.605]
#0.1
loss1=[1.187, 1.096, 1.044, 1.01, 0.982, 0.958, 0.927, 0.912, 0.887, 0.867, 0.83, 0.833, 0.825, 0.809, 0.783, 0.79, 0.78, 0.781, 0.773, 0.746, 0.751, 0.733, 0.73, 0.725, 0.718, 0.703, 0.712, 0.702, 0.688, 0.68, 0.692, 0.677, 0.67, 0.675, 0.662, 0.653, 0.652, 0.653, 0.65, 0.647, 0.641, 0.642, 0.637, 0.62, 0.631, 0.632, 0.624, 0.614, 0.618, 0.606]
#0.2
loss2=[]
#0.3
loss3=[1.185, 1.076, 1.032, 0.978, 0.96, 0.925, 0.902, 0.884, 0.869, 0.849, 0.826, 0.809, 0.794, 0.797, 0.773, 0.779, 0.764, 0.737, 0.731, 0.723, 0.73, 0.714, 0.698, 0.714, 0.694, 0.687, 0.686, 0.674, 0.675, 0.66, 0.663, 0.654, 0.641, 0.647, 0.626, 0.633, 0.63, 0.634, 0.63, 0.616, 0.631, 0.621, 0.603, 0.611, 0.608, 0.599, 0.593, 0.583, 0.598, 0.592]
#0.4
loss4=[1.153, 1.093, 1.099, 1.07, 1.073, 1.06, 1.061, 1.046, 1.038, 1.022, 1.019, 0.998, 0.997, 0.985, 0.983, 0.987, 0.97, 0.969, 0.965, 0.953, 0.951, 0.942, 0.941, 0.924, 0.94, 0.937, 0.921, 0.913, 0.912, 0.908, 0.905, 0.896, 0.901, 0.881, 0.883, 0.875, 0.877, 0.868, 0.879, 0.874, 0.863, 0.84, 0.862, 0.875, 0.849, 0.851, 0.848, 0.851, 0.848, 0.853, 0.832, 0.829, 0.812, 0.836, 0.815, 0.837, 0.843, 0.818, 0.828, 0.821, 0.831, 0.824, 0.809, 0.822, 0.813, 0.82, 0.801, 0.794, 0.798, 0.801, 0.802, 0.798, 0.822, 0.772, 0.801, 0.786, 0.775, 0.782, 0.792, 0.777, 0.79, 0.802, 0.799, 0.777, 0.799, 0.774, 0.753, 0.77, 0.783, 0.756, 0.776, 0.771, 0.761, 0.769, 0.774, 0.747, 0.768, 0.772, 0.762, 0.774]
#0.5
loss55=[1.176, 1.133, 1.122, 1.106, 1.112, 1.063, 1.073, 1.066, 1.056, 1.041, 1.042, 1.031, 1.014, 1.003, 1.018, 0.984, 0.988, 0.997, 0.997, 0.977, 0.982, 0.972, 0.977, 0.953, 0.957, 0.951, 0.963, 0.935, 0.93, 0.929, 0.921, 0.926, 0.941, 0.915, 0.908, 0.92, 0.905, 0.896, 0.905, 0.904, 0.897, 0.895, 0.879, 0.883, 0.875, 0.876, 0.896, 0.873, 0.859, 0.879, 0.857, 0.869, 0.852, 0.861, 0.856, 0.853, 0.868, 0.838, 0.858, 0.829, 0.858, 0.86, 0.845, 0.84, 0.837, 0.848, 0.833, 0.835, 0.821, 0.817, 0.809, 0.829, 0.819, 0.822, 0.822, 0.821, 0.827, 0.813, 0.832, 0.804, 0.831, 0.81, 0.818, 0.816, 0.805, 0.812, 0.809, 0.811, 0.808, 0.803, 0.806, 0.814, 0.794, 0.826, 0.786, 0.782, 0.796, 0.807, 0.83, 0.786]
#0.6
loss6=[1.19, 1.094, 1.033, 0.98, 0.933, 0.897, 0.881, 0.842, 0.83, 0.803, 0.781, 0.775, 0.767, 0.74, 0.726, 0.72, 0.727, 0.71, 0.679, 0.692, 0.675, 0.663, 0.656, 0.662, 0.652, 0.666, 0.646, 0.636, 0.64, 0.617, 0.623, 0.625, 0.624, 0.611, 0.609, 0.602, 0.599, 0.583, 0.583, 0.588, 0.575, 0.581, 0.567, 0.579, 0.573, 0.571, 0.565, 0.578, 0.567, 0.562]
#0.7
loss7=[1.147, 1.046, 0.984, 0.95, 0.901, 0.886, 0.849, 0.848, 0.824, 0.808, 0.784, 0.763, 0.739, 0.766, 0.737, 0.742, 0.725, 0.708, 0.706, 0.685, 0.67, 0.669, 0.666, 0.68, 0.652, 0.648, 0.642, 0.648, 0.624, 0.623, 0.635, 0.627, 0.603, 0.618, 0.605, 0.593, 0.599, 0.594, 0.604, 0.596, 0.595, 0.582, 0.586, 0.586, 0.572, 0.558, 0.576, 0.551, 0.557, 0.561]


alexnet_acc7=[0.85, 0.809, 0.89, 0.914, 0.942, 0.928, 0.866, 0.938, 0.955, 0.948, 0.957, 0.949, 0.965, 0.949, 0.954, 0.97, 0.969, 0.964, 0.952, 0.963, 0.963, 0.951, 0.967, 0.948, 0.969, 0.963, 0.959, 0.972, 0.971, 0.934, 0.966, 0.964, 0.967, 0.965, 0.948, 0.958, 0.963, 0.969, 0.955, 0.971, 0.967, 0.97, 0.971, 0.963, 0.971, 0.969, 0.961, 0.975, 0.97, 0.97]
alexnetse_acc7=[0.84, 0.848, 0.897, 0.884, 0.936, 0.923, 0.943, 0.945, 0.955, 0.925, 0.937, 0.952, 0.955, 0.958, 0.955, 0.901, 0.949, 0.966, 0.936, 0.976, 0.963, 0.973, 0.963, 0.97, 0.967, 0.954, 0.964, 0.973, 0.978, 0.97, 0.971, 0.965, 0.978, 0.964, 0.97, 0.945, 0.936, 0.978, 0.971, 0.971, 0.972, 0.964, 0.977, 0.982, 0.978, 0.959, 0.978, 0.975, 0.971, 0.972]


resnet_acc7=[0.531, 0.489, 0.567, 0.606, 0.633, 0.559, 0.507, 0.561, 0.552, 0.682, 0.703, 0.72, 0.671, 0.759, 0.679, 0.608, 0.739, 0.741, 0.729, 0.745, 0.758, 0.759, 0.779, 0.774, 0.764, 0.727, 0.773, 0.742, 0.754, 0.737, 0.779, 0.756, 0.76, 0.777, 0.758, 0.807, 0.748, 0.755, 0.768, 0.765, 0.773, 0.8, 0.802, 0.791, 0.77, 0.788, 0.799, 0.773, 0.762, 0.784]
resnetse_acc7=[0.526, 0.589, 0.727, 0.7, 0.671, 0.741, 0.739, 0.744, 0.791, 0.789, 0.758, 0.765, 0.752, 0.772, 0.783, 0.778, 0.805, 0.735, 0.788, 0.8, 0.8, 0.811, 0.762, 0.825, 0.806, 0.825, 0.805, 0.801, 0.801, 0.82, 0.802, 0.831, 0.834, 0.808, 0.836, 0.818, 0.832, 0.776, 0.848, 0.823, 0.835, 0.831, 0.817, 0.825, 0.854, 0.856, 0.858, 0.847, 0.849, 0.843]


loss8=[0.85, 0.514, 0.394, 0.391, 0.319, 0.288, 0.272, 0.271, 0.262, 0.238, 0.232, 0.211, 0.201, 0.195, 0.183, 0.179, 0.212, 0.197, 0.185, 0.203, 0.18, 0.164, 0.167, 0.165, 0.159, 0.147, 0.163, 0.164, 0.147, 0.136, 0.152, 0.157, 0.159, 0.174, 0.159, 0.151, 0.137, 0.138, 0.157, 0.148, 0.138, 0.201, 0.144, 0.126, 0.124, 0.133, 0.124, 0.117, 0.105, 0.131]
loss9 = [0.985, 0.757, 0.748, 0.723, 0.739, 0.635, 0.573, 0.607, 0.61, 0.545, 0.473, 0.478, 0.518, 0.517, 0.482, 0.604, 0.906, 0.837, 0.686, 0.65, 0.702, 0.641, 0.634, 0.57, 0.675, 0.625, 0.522, 0.688, 0.573, 0.569, 0.552, 0.704, 0.732, 0.777, 1.322, 1.306, 1.327, 1.323, 1.317, 1.358, 1.331, 1.331, 1.33, 1.332, 1.326, 1.33, 1.331, 1.33, 1.329, 1.326]


Alex=[91.91,92.27,92.65,92.65,92.52,92.40,92.28,93.01]
AlexSE=[87.99,88.73,91.78,93.75,95.34,95.83,95.83,96.32]
resnet50=[80.27,80.64,82.72,83.58,81.62,82.48,82.11,83.70]
resnet50se = [85.17,85.17,85.42,86.03,86.27,86.76,87.50,87.75]

sampleRate = [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7]


plt.plot(sampleRate,Alex,label='CS-AlexNet',marker='v')
plt.plot(sampleRate,AlexSE,label='Proposed',marker='s')
plt.plot(sampleRate,resnet50,label='ResNet50',marker='p')
plt.plot(sampleRate,resnet50se,label='ResNet50+SE',marker='h')

#plt.title('')
plt.xlabel('Sensing Rate')
plt.ylabel('Accuracy(%)')

plt.legend()

plt.show()